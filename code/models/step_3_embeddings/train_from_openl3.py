import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler # Added StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib

# --- Configuration ---
# Determine ROOT_DIR based on the script's location
# Assuming this script is in a subdirectory of your main project (e.g., a 'scripts' or 'training' folder)
current_file_path = os.path.abspath(__file__)
SCRIPTS_DIR = os.path.dirname(current_file_path)
ROOT_DIR = os.path.dirname(SCRIPTS_DIR) # Assumes this script is one level down from project root

# Path to the CSV file generated by extract_openl3_embeddings.py
# Ensure this path matches where extract_openl3_embeddings_v2 saves its output
FEATURES_DATA_DIR = os.path.join(ROOT_DIR, 'step_3_embeddings', 'openl3_features')
EMBEDDING_CSV = os.path.join(FEATURES_DATA_DIR, 'openl3_features.csv')

# Output directory for results (models, reports, plots)
# This will create 'results_openl3_real_is_1' at the project root

OUTPUT_DIR_BASE_NAME = "results" 
OUTPUT_DIR = os.path.join(ROOT_DIR, 'step_3_embeddings', OUTPUT_DIR_BASE_NAME)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Load Data ---
print(f"Loading embeddings from: {EMBEDDING_CSV}")
if not os.path.exists(EMBEDDING_CSV):
    print(f"ERROR: Embedding CSV file not found at {EMBEDDING_CSV}")
    print("Please run the extract_openl3_embeddings.py script first.")
    exit()

df = pd.read_csv(EMBEDDING_CSV)

# Convert feature strings back to numpy arrays
print("Converting feature strings to numpy arrays...")
X = np.vstack(df['features'].str.split(',').apply(lambda x: list(map(float, x))))
y = df['label'].values # Real=1, Fake=0
groups = df['group'].values # For GroupShuffleSplit

print(f"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features per sample.")
print("Label distribution in loaded data:")
print(pd.Series(y).value_counts())

# --- Train-Test Split (Group Aware) ---
print("Performing group-aware train-test split...")
splitter = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42) # 70% train, 30% test
train_idx, test_idx = next(splitter.split(X, y, groups=groups))

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")
print("Label distribution in training set:")
print(pd.Series(y_train).value_counts())
print("Label distribution in test set:")
print(pd.Series(y_test).value_counts())

# --- Feature Scaling ---
print("Applying StandardScaler to features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test) # Use the scaler fitted on training data

# --- Define Models ---
# Using your preferred convention: Real=1, Fake=0
# target_names for classification_report will be ['Fake', 'Real']
target_names_report = ['Fake (0)', 'Real (1)']

models = {
    "random_forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced_subsample'),
    "svm": SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced'), # probability=True needed for predict_proba
    "mlp": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42, early_stopping=True, n_iter_no_change=10),
    "logistic": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', solver='liblinear')
}

# --- Train and Evaluate Models ---
for model_name, model_instance in models.items():
    print(f"\n▶️ Training {model_name}...")
    
    # Create a specific directory for this model's results
    current_model_output_dir = os.path.join(OUTPUT_DIR, model_name)
    os.makedirs(current_model_output_dir, exist_ok=True)
    
    # Fit the model on SCALED training data
    model_instance.fit(X_train_scaled, y_train)
    
    # Make predictions on SCALED test data
    y_pred = model_instance.predict(X_test_scaled)
    
    # Generate and print classification report
    report_dict = classification_report(y_test, y_pred, target_names=target_names_report, output_dict=True, zero_division=0)
    report_str = classification_report(y_test, y_pred, target_names=target_names_report, zero_division=0)
    print(f"\nClassification Report for {model_name}:\n", report_str)
    
    # Save the text report
    with open(os.path.join(current_model_output_dir, "classification_report.txt"), "w") as f:
        f.write(f"Classification Report for {model_name}:\n")
        f.write(report_str)
        f.write("\n\n--- Dictionary Format ---\n")
        for k, v_dict in report_dict.items(): # Iterate through outer keys (class labels, accuracy, etc.)
            f.write(f"\n{k}:\n")
            if isinstance(v_dict, dict): # Check if the value is a dictionary of metrics
                for metric_name, metric_value in v_dict.items():
                    f.write(f"  {metric_name}: {metric_value}\n")
            else: # For simple values like accuracy
                f.write(f"  value: {v_dict}\n")

    # Generate and save confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=[0, 1]) # Explicitly set labels for CM order
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues_r', 
                xticklabels=target_names_report, yticklabels=target_names_report)
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig(os.path.join(current_model_output_dir, "confusion_matrix.png"))
    plt.close() # Close the plot to free memory

    # Save the trained model AND the scaler
    model_and_scaler_bundle = {
        "model": model_instance,
        "scaler": scaler # The scaler fitted on X_train
    }
    joblib.dump(model_and_scaler_bundle, os.path.join(current_model_output_dir, "model_and_scaler.joblib"))
    print(f"Model and Scaler for {model_name} saved to {current_model_output_dir}")

print(f"\n✅ All models trained and results saved to: {OUTPUT_DIR}")
